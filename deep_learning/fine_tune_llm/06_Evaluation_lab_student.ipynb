{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYOHSHAZ4nYEliH7vgQ0ai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmnchopra91/gen-ai/blob/main/deep_learning/fine_tune_llm/06_Evaluation_lab_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "XNcarmxMxAgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Technically, there are very few steps to run it on GPUs, elsewhere (ie. on Lamini).\n",
        "```\n",
        "finetuned_model = BasicModelRunner(\n",
        "    \"lamini/lamini_docs_finetuned\"\n",
        ")\n",
        "finetuned_output = finetuned_model(\n",
        "    test_dataset_list # batched!\n",
        ")\n",
        "```\n",
        "\n",
        "### Let's look again under the hood! This is the open core code of Lamini's `llama` library :)"
      ],
      "metadata": {
        "id": "KcTqfzQ8xDxc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf8XyBn0w5oy"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import tempfile\n",
        "import logging\n",
        "import random\n",
        "import config\n",
        "import os\n",
        "import yaml\n",
        "import logging\n",
        "import difflib\n",
        "import pandas as pd\n",
        "\n",
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "from utilities import *\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "global_config = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\"lamini/lamini_docs\")\n",
        "\n",
        "test_dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "aNjvtAO_xRbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_dataset[0][\"question\"])\n",
        "print(test_dataset[0][\"answer\"])"
      ],
      "metadata": {
        "id": "pkw-19dkxTZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"lamini/lamini_docs_finetuned\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "MZ8W_w7oxVBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup a really basic evaluation function"
      ],
      "metadata": {
        "id": "D6LbCYfXxXQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_exact_match(a, b):\n",
        "    return a.strip() == b.strip()"
      ],
      "metadata": {
        "id": "Duckdv7GxYFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "efWzW-4mxaIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  input_ids = tokenizer.encode(\n",
        "      text,\n",
        "      return_tensors=\"pt\",\n",
        "      truncation=True,\n",
        "      max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "nyXHt635xdNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run model and compare to expected answer"
      ],
      "metadata": {
        "id": "JHVkYZ3WxfgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = test_dataset[0][\"question\"]\n",
        "generated_answer = inference(test_question, model, tokenizer)\n",
        "print(test_question)\n",
        "print(generated_answer)"
      ],
      "metadata": {
        "id": "MY2Cq2xoxhB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = test_dataset[0][\"answer\"]\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "SfZ_KZPfxkFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exact_match = is_exact_match(generated_answer, answer)\n",
        "print(exact_match)"
      ],
      "metadata": {
        "id": "Qj_nI8alxmK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run over entire dataset"
      ],
      "metadata": {
        "id": "0O6LV8g9xoXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "metrics = {'exact_matches': []}\n",
        "predictions = []\n",
        "for i, item in tqdm(enumerate(test_dataset)):\n",
        "    print(\"i Evaluating: \" + str(item))\n",
        "    question = item['question']\n",
        "    answer = item['answer']\n",
        "\n",
        "    try:\n",
        "      predicted_answer = inference(question, model, tokenizer)\n",
        "    except:\n",
        "      continue\n",
        "    predictions.append([predicted_answer, answer])\n",
        "\n",
        "    #fixed: exact_match = is_exact_match(generated_answer, answer)\n",
        "    exact_match = is_exact_match(predicted_answer, answer)\n",
        "    metrics['exact_matches'].append(exact_match)\n",
        "\n",
        "    if i > n and n != -1:\n",
        "      break\n",
        "print('Number of exact matches: ', sum(metrics['exact_matches']))"
      ],
      "metadata": {
        "id": "v82vxusDxq3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(predictions, columns=[\"predicted_answer\", \"target_answer\"])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "-jL8hnoTxtQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate all the data"
      ],
      "metadata": {
        "id": "aV1L2S_nxvb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset_path = \"lamini/lamini_docs_evaluation\"\n",
        "evaluation_dataset = datasets.load_dataset(evaluation_dataset_path)"
      ],
      "metadata": {
        "id": "wV8-H_VtxyFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(evaluation_dataset)"
      ],
      "metadata": {
        "id": "yTKkmwuJxztr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try the ARC benchmark\n",
        "This can take several minutes"
      ],
      "metadata": {
        "id": "g95SDku_x18H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python lm-evaluation-harness/main.py --model hf-causal --model_args pretrained=lamini/lamini_docs_finetuned --tasks arc_easy --device cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Bhl670x3BW",
        "outputId": "ade531a7-535e-4332-be4f-13c4805484c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/lm-evaluation-harness/main.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuWaORBDx5aW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}